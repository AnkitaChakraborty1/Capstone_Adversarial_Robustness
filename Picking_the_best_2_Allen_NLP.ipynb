{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkitaChakraborty1/Capstone_Adversarial_Robustness/blob/main/Picking_the_best_2_Allen_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pip installs"
      ],
      "metadata": {
        "id": "qFwazax85sRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ByQttbCVWzZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a826a099-648e-4ab3-944b-0ea991a76893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gcp6ZZGvVk_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25eea909-8021-415e-f5e5-c4d12ee6adaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack[tensorflow]\n",
            "  Downloading textattack-0.3.4-py3-none-any.whl (373 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 71 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 92 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 348 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 358 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 368 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 373 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 14.3 MB/s \n",
            "\u001b[?25hCollecting lru-dict\n",
            "  Downloading lru-dict-1.1.7.tar.gz (10 kB)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.3.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (8.12.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.21.5)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (3.2.5)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.4.1)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.7.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 89.9 MB/s \n",
            "\u001b[?25hCollecting lemminflect\n",
            "  Downloading lemminflect-0.2.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 98.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (3.6.0)\n",
            "Collecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 81.7 MB/s \n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (4.17.0)\n",
            "Collecting tensorflow-estimator==2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 87.1 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.5.0\n",
            "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 17 kB/s \n",
            "\u001b[?25hCollecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from textattack[tensorflow]) (0.12.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.15.0)\n",
            "Collecting numpy>=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.6.3)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.37.1)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 89.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (0.2.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[tensorflow]) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (21.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0->textattack[tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score>=0.3.5->textattack[tensorflow]) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2.8.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[tensorflow]) (3.2.0)\n",
            "Collecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 81.5 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 53.1 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 66.8 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[tensorflow]) (0.11.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 88.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 96.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 94.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 74.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 73.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 77.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 74.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 92.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 76.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 82.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 98.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 72.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 73.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 72.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 69.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 74.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 63.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (6.0.1)\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 48.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.1-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 72.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 70.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 65.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 63.9 MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 95.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 96.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 84.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.0-py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 85.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 100.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 105.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[tensorflow]) (0.3.4)\n",
            "  Downloading datasets-1.4.0-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 99.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 87.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 99.6 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 101.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (4.2.6)\n",
            "Collecting more-itertools\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (1.0.2)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 363 kB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 80.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (0.8.9)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack[tensorflow]) (3.6.0)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 80.0 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.8 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair->textattack[tensorflow]) (5.2.1)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.4-py3-none-any.whl (19 kB)\n",
            "  Downloading konoha-4.6.3-py3-none-any.whl (19 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair->textattack[tensorflow]) (0.2.5)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack[tensorflow]) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack[tensorflow]) (7.1.2)\n",
            "Building wheels for collected packages: wrapt, gdown, mpld3, overrides, sqlitedict, langdetect, lru-dict, wikipedia-api, word2number\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68718 sha256=1f4cf08d54d87bc0cf39876144a181e6769fa6208382278a1f15a50f7eeb433b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=51bf666db111ac7a5643b1199cc8f4f25daa00c321a2c76bc2d4e623008e8f8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=d6fafc05a9d5866d610f3eac2f49e33176ff287a06f692da776f20cc27547075\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=9ab16fb5f0ab61ce53f47502fd8319f796e8dfdb847bf3eeffbc32c12046d6e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=53eb08997fc8eecaf18fc0e6e582c3ac8614c63622645fec9e9a17218f96be5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=daf5d1d4c2e39a2b3ba8a1236af2e46c12bd6c6b7d7943ddbe2149b84f289bdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp37-cp37m-linux_x86_64.whl size=28419 sha256=e49314768d2d32f5fffaa7eb9ad03d53f480fcb9cb4e4a277b14625aa2e3e316\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/0b/4e/aa8fec9833090cd52bcd76f92f9d95e1ee7b915c12093663b4\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=6c3aad2ebc9aeb6baba64b094dbda17ec51db08267e95a10ad5413c172ec7132\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=edf2a48afff4cfac3270f18921f2f049256f73431ab36c9b2707eb1dd0bada94\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built wrapt gdown mpld3 overrides sqlitedict langdetect lru-dict wikipedia-api word2number\n",
            "Installing collected packages: typing-extensions, numpy, tqdm, wrapt, sentencepiece, overrides, grpcio, absl-py, xxhash, wikipedia-api, tensorflow-estimator, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, keras-nightly, janome, gdown, gast, ftfy, flatbuffers, deprecated, conllu, bpemb, word2number, terminaltables, tensorflow, num2words, lru-dict, lemminflect, language-tool-python, flair, datasets, bert-score, textattack, tensorflow-text, tensorboardX\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.63.0\n",
            "    Uninstalling tqdm-4.63.0:\n",
            "      Successfully uninstalled tqdm-4.63.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.0\n",
            "    Uninstalling wrapt-1.14.0:\n",
            "      Successfully uninstalled wrapt-1.14.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.44.0\n",
            "    Uninstalling grpcio-1.44.0:\n",
            "      Successfully uninstalled grpcio-1.44.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 bert-score-0.3.11 bpemb-0.3.3 conllu-4.4.1 datasets-1.2.1 deprecated-1.2.13 flair-0.10 flatbuffers-1.12 ftfy-6.1.1 gast-0.4.0 gdown-3.12.2 grpcio-1.34.1 janome-0.4.2 keras-nightly-2.5.0.dev2021032900 konoha-4.6.3 langdetect-1.0.9 language-tool-python-2.7.0 lemminflect-0.2.2 lru-dict-1.1.7 more-itertools-8.8.0 mpld3-0.3 num2words-0.5.10 numpy-1.19.5 overrides-3.1.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tensorboardX-2.5 tensorflow-2.5.0 tensorflow-estimator-2.5.0 tensorflow-text-2.5.0 terminaltables-3.1.10 textattack-0.3.4 tqdm-4.49.0 typing-extensions-3.7.4.3 wikipedia-api-0.5.4 word2number-1.1 wrapt-1.12.1 xxhash-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install textattack[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "KNCInb7pfKEt",
        "outputId": "9e1e005e-1dca-4794-8cfa-35ac1f4ec1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 7.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.21.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNCaMfyjeEcP",
        "outputId": "53445d69-ae08-4c4e-f239-2702cbf067d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 37.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=4895495c70ea50379ed2922b250333c3c4fcbceeb29d663761b4a3c2a17b43fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install folium==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbhNJJBQcpmy",
        "outputId": "7f78411d-ba13-44c6-aad4-95875ffcc9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.9.2-py3-none-any.whl (719 kB)\n",
            "\u001b[K     |████████████████████████████████| 719 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting allennlp_models\n",
            "  Downloading allennlp_models-2.9.0-py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 23.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.5)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: transformers<4.18,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.95)\n",
            "Requirement already satisfied: torch<1.12.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.10.0+cu111)\n",
            "Collecting cached-path<1.2.0,>=1.0.2\n",
            "  Downloading cached_path-1.1.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: torchvision<0.13.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.11.1+cu111)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: spacy<3.3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 83.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.0)\n",
            "Collecting fairscale==0.4.6\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 72.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
            "\u001b[K     |████████████████████████████████| 592 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.21.5)\n",
            "Requirement already satisfied: filelock<3.7,>=3.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting tqdm>=4.62\n",
            "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<1.2.0,>=1.0.2->allennlp) (1.18.1)\n",
            "Collecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.21.32-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.25.0,>=1.24.32\n",
            "  Downloading botocore-1.24.32-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.32->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 95.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (4.2.4)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.56.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 96.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.10.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.0.16->allennlp) (3.7.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.13.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.18,>=4.1->allennlp) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18,>=4.1->allennlp) (0.11.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.8-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 79.9 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: conllu==4.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (4.4.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (6.1.1)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (1.2.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 63.1 MB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp) (1.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (0.70.12.2)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 67.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Using cached fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 79.8 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 95.3 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (2.0.12)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp_models) (0.2.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (3.1.0)\n",
            "Building wheels for collected packages: fairscale, jsonnet, pathtools\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=1e40949e15de8fa3c3043d77a45598befe33e255ab981e3a5308b53f35406dfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994560 sha256=d41a509ccf114c658b350133bb7c651fe132581503ce6ea8d6f4157c097c9742\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ce42c390dcd97d4ba2750ba703e25f1af8af39d1cdbaa40f49d345ed58a026a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built fairscale jsonnet pathtools\n",
            "Installing collected packages: urllib3, jmespath, smmap, multidict, frozenlist, botocore, yarl, tqdm, s3transfer, regex, gitdb, asynctest, async-timeout, aiosignal, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, fsspec, docker-pycreds, boto3, aiohttp, wandb, responses, nltk, jsonnet, fairscale, cached-path, base58, py-rouge, datasets, allennlp, allennlp-models\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.49.0\n",
            "    Uninstalling tqdm-4.49.0:\n",
            "      Successfully uninstalled tqdm-4.49.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 1.2.1\n",
            "    Uninstalling datasets-1.2.1:\n",
            "      Successfully uninstalled datasets-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textattack 0.3.4 requires tqdm<4.50.0,>=4.27, but you have tqdm 4.63.1 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 aiohttp-3.8.1 aiosignal-1.2.0 allennlp-2.9.2 allennlp-models-2.9.0 async-timeout-4.0.2 asynctest-0.13.0 base58-2.1.1 boto3-1.21.32 botocore-1.24.32 cached-path-1.1.1 datasets-2.0.0 docker-pycreds-0.4.0 fairscale-0.4.6 frozenlist-1.3.0 fsspec-2022.3.0 gitdb-4.0.9 jmespath-1.0.0 jsonnet-0.18.0 multidict-6.0.2 nltk-3.7 pathtools-0.1.2 py-rouge-1.1 regex-2022.3.15 responses-0.18.0 s3transfer-0.5.2 sentry-sdk-1.5.8 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 tqdm-4.63.1 urllib3-1.25.11 wandb-0.12.11 yarl-1.7.2 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install allennlp allennlp_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TD0japgycmeU",
        "outputId": "b26b5fa4-81e8-41d8-92e2-760c1f706820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (1.18.1)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
            "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (2.23.0)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
            "  Downloading google_api_core-2.7.1-py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 96.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.35.0)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.2.3-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.25.11)\n",
            "Installing collected packages: google-crc32c, google-api-core, google-resumable-media, google-cloud-core, google-cloud-storage\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.7.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.3 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.7.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.7.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.3 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.7.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.3 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.3 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.7.1 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed google-api-core-2.7.1 google-cloud-core-2.2.3 google-cloud-storage-2.2.1 google-crc32c-1.3.0 google-resumable-media-2.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrq7Mk7seR2N"
      },
      "source": [
        "## Creating the AllenNLP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFmEbx32cgRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b47d03-74fc-4b08-ca24-52af88d59a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:11<00:00, 43.4MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmp4ljw7wcj.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n"
          ]
        }
      ],
      "source": [
        "from allennlp.predictors import Predictor\n",
        "import allennlp_models.classification\n",
        "\n",
        "import textattack\n",
        "\n",
        "class AllenNLPModel(textattack.models.wrappers.ModelWrapper):\n",
        "    def __init__(self):\n",
        "        self.predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/basic_stanford_sentiment_treebank-2020.06.09.tar.gz\")\n",
        "        self.model = self.predictor._model\n",
        "        self.tokenizer = self.predictor._dataset_reader._tokenizer\n",
        "\n",
        "    def __call__(self, text_input_list):\n",
        "        outputs = []\n",
        "        for text_input in text_input_list:\n",
        "            outputs.append(self.predictor.predict(sentence=text_input))\n",
        "        # For each output, outputs['logits'] contains the logits where\n",
        "        # index 0 corresponds to the positive and index 1 corresponds \n",
        "        # to the negative score. We reverse the outputs (by reverse slicing,\n",
        "        # [::-1]) so that negative comes first and positive comes second.\n",
        "        return [output['logits'][::-1] for output in outputs]\n",
        "\n",
        "#model_wrapper = AllenNLPModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQowlAHSL4fC",
        "outputId": "323fe005-d01c-4f24-93ce-732605e8a174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Building wheels for collected packages: torchfile\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=495e57a038bb92dc09cfe292c8fddf519c434be70cf9748708d1fe4ca6757621\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built torchfile\n",
            "Installing collected packages: torchfile\n",
            "Successfully installed torchfile-0.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrapper = AllenNLPModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsZHNB24ZMN0",
        "outputId": "61cddd06-66b9-4afc-ffad-e7f99f17bf8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "downloading: 100%|##########| 35.3M/35.3M [00:00<00:00, 57.0MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack import AttackArgs, Attacker\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.models.wrappers import ModelWrapper\n",
        "from textattack import Attack\n",
        "from textattack.constraints.pre_transformation import (\n",
        "        InputColumnModification,\n",
        "        MaxModificationRate,\n",
        "        RepeatModification,\n",
        "        StopwordModification,\n",
        "        MaxWordIndexModification\n",
        "                )\n",
        "from textattack.constraints.semantics import WordEmbeddingDistance\n",
        "from textattack.goal_functions import UntargetedClassification, InputReduction\n",
        "from textattack.search_methods import ImprovedGeneticAlgorithm, GreedyWordSwapWIR\n",
        "from textattack.transformations import WordDeletion, WordSwapHomoglyphSwap, WordSwapNeighboringCharacterSwap\n",
        "from textattack.constraints.overlap import LevenshteinEditDistance\n",
        "#from textattack.attack_recipe import AttackRecipe\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.attacker import Attacker\n",
        "from textattack import AttackArgs, Attacker\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.goal_functions import TargetedClassification\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "torch.multiprocessing.freeze_support()\n",
        "dataset = HuggingFaceDataset(\"glue\", \"sst2\", \"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "5c81526aaa3a4671bbe777f9b6c0a918",
            "c23a3c3f54f146a7b43d58608f0962ff",
            "8c2b1c6115164a239f7f3021a6100b5f",
            "23d7702ae07f40d2ab72ff27b0fdb359",
            "7122d9e6bd8442b2b7dd7142c0943a3e",
            "c1a78dbd19914a498b865c541137a78c",
            "bd2402f291eb47feba67a130fd088c0c",
            "30f4958b6e8547a9810adf67c6844569",
            "61e0b489a3e6400f95f3cc1eecec1eb6",
            "3179723d6a5a4c1f98868ef133638c68",
            "1d6c84b54a8c44e3b678c14767f1cc1e"
          ]
        },
        "id": "DADuNfu8xI9A",
        "outputId": "a01329c5-22b5-48ca-febf-19be0d4037a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c81526aaa3a4671bbe777f9b6c0a918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mglue\u001b[0m, subset \u001b[94msst2\u001b[0m, split \u001b[94mtrain\u001b[0m.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Recipe1(Attack,ABC):\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordDeletion()\n",
        "\n",
        "        constraints = [MaxWordIndexModification(5)]\n",
        "\n",
        "        goal_function = InputReduction(model_wrapper)\n",
        "        # search over words based on a combination of their saliency score, and how efficient the WordSwap transform is\n",
        "        search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
        "        return Attack(goal_function, constraints, transformation, search_method)"
      ],
      "metadata": {
        "id": "RLdzkoQUwNCB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack = Recipe1.build(model_wrapper)\n",
        "attacker = Attacker(attack, dataset)\n",
        "attacker.attack_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvIkllay1IH0",
        "outputId": "7fd25850-c6ab-42c5-a897-0cc9ebff02a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'allennlp.models.basic_classifier.BasicClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.input_reduction.InputReduction'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  InputReduction(\n",
            "    (maximizable):  False\n",
            "    (target_num_words):  1\n",
            "  )\n",
            "  (transformation):  WordDeletion\n",
            "  (constraints): \n",
            "    (0): MaxWordIndexModification(\n",
            "        (max_length):  5\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 3 / 1 / 4:  40%|████      | 4/10 [00:00<00:01,  4.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Negative (95%)]] --> [[[FAILED]]]\n",
            "\n",
            "hide new secretions from the parental units \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[Negative (96%)]] --> [[[FAILED]]]\n",
            "\n",
            "contains no wit , only labored gags \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[Positive (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "that loves its characters and communicates something rather beautiful about human nature \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[Positive (82%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "remains utterly satisfied to remain the same throughout \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 5 / 1 / 6:  60%|██████    | 6/10 [00:00<00:00,  6.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[Negative (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "that 's far too tragic to merit such superficial treatment \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 1 / 8 / 1 / 10: 100%|██████████| 10/10 [00:01<00:00,  7.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[Positive (90%)]] --> [[Positive (88%)]]\n",
            "\n",
            "[[of]] saucy \n",
            "\n",
            "saucy \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "a depressed fifteen-year-old 's suicidal poetry \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[Positive (79%)]] --> [[[FAILED]]]\n",
            "\n",
            "are more deeply thought through than in most ` right-thinking ' films \n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 1      |\n",
            "| Number of failed attacks:     | 8      |\n",
            "| Number of skipped attacks:    | 1      |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 80.0%  |\n",
            "| Attack success rate:          | 11.11% |\n",
            "| Average perturbed word %:     | 50.0%  |\n",
            "| Average num. words per input: | 9.5    |\n",
            "| Avg num queries:              | 15.0   |\n",
            "+-------------------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe970602090>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9701ca410>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97436ea50>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7fe97436ea10>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe96a426250>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe96df49550>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9730bf510>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fe9730bf5d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9701cacd0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97436e650>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Recipe2(Attack,ABC):\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordDeletion()\n",
        "\n",
        "        constraints = [MaxWordIndexModification(5)]\n",
        "\n",
        "        goal_function = InputReduction(model_wrapper)\n",
        "        # search over words based on a combination of their saliency score, and how efficient the WordSwap transform is\n",
        "        search_method = GreedyWordSwapWIR(wir_method=\"weighted-saliency\")\n",
        "        return Attack(goal_function, constraints, transformation, search_method)\n",
        "attack = Recipe2.build(model_wrapper)\n",
        "attacker = Attacker(attack, dataset)\n",
        "attacker.attack_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00LpzqAzH3IQ",
        "outputId": "79af6df8-97b1-450b-b5f7-4f27f1826ba1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'allennlp.models.basic_classifier.BasicClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.input_reduction.InputReduction'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  weighted-saliency\n",
            "  )\n",
            "  (goal_function):  InputReduction(\n",
            "    (maximizable):  False\n",
            "    (target_num_words):  1\n",
            "  )\n",
            "  (transformation):  WordDeletion\n",
            "  (constraints): \n",
            "    (0): MaxWordIndexModification(\n",
            "        (max_length):  5\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|██        | 2/10 [00:00<00:00, 15.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Negative (95%)]] --> [[[FAILED]]]\n",
            "\n",
            "hide new secretions from the parental units \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[Negative (96%)]] --> [[[FAILED]]]\n",
            "\n",
            "contains no wit , only labored gags \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 3 / 1 / 4:  40%|████      | 4/10 [00:00<00:00, 17.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[Positive (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "that loves its characters and communicates something rather beautiful about human nature \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[Positive (82%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "remains utterly satisfied to remain the same throughout \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 5 / 1 / 6:  60%|██████    | 6/10 [00:00<00:00, 14.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[Negative (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "that 's far too tragic to merit such superficial treatment \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 1 / 8 / 1 / 10: 100%|██████████| 10/10 [00:00<00:00, 12.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[Positive (90%)]] --> [[Positive (88%)]]\n",
            "\n",
            "[[of]] saucy \n",
            "\n",
            "saucy \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "a depressed fifteen-year-old 's suicidal poetry \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[Positive (79%)]] --> [[[FAILED]]]\n",
            "\n",
            "are more deeply thought through than in most ` right-thinking ' films \n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 1      |\n",
            "| Number of failed attacks:     | 8      |\n",
            "| Number of skipped attacks:    | 1      |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 80.0%  |\n",
            "| Attack success rate:          | 11.11% |\n",
            "| Average perturbed word %:     | 50.0%  |\n",
            "| Average num. words per input: | 9.5    |\n",
            "| Avg num queries:              | 19.67  |\n",
            "+-------------------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe975cce990>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe96a426e50>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7feb93118ad0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7fe9740d5610>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7feb931148d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe972769a90>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97fde2410>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7feb9312f290>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97fde2a50>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7feb93118a50>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Recipe3(Attack,ABC):\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordSwapHomoglyphSwap()\n",
        "\n",
        "        constraints = [MaxWordIndexModification(5)]\n",
        "\n",
        "        goal_function = InputReduction(model_wrapper)\n",
        "        # search over words based on a combination of their saliency score, and how efficient the WordSwap transform is\n",
        "        search_method = GreedyWordSwapWIR(wir_method=\"weighted-saliency\")\n",
        "        return Attack(goal_function, constraints, transformation, search_method)\n",
        "attack = Recipe3.build(model_wrapper)\n",
        "attacker = Attacker(attack, dataset)\n",
        "attacker.attack_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeJxJp03IJv3",
        "outputId": "24bb4d43-6dbd-4d32-cc14-4767808f5b05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'allennlp.models.basic_classifier.BasicClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.input_reduction.InputReduction'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  weighted-saliency\n",
            "  )\n",
            "  (goal_function):  InputReduction(\n",
            "    (maximizable):  False\n",
            "    (target_num_words):  1\n",
            "  )\n",
            "  (transformation):  WordSwapHomoglyphSwap\n",
            "  (constraints): \n",
            "    (0): MaxWordIndexModification(\n",
            "        (max_length):  5\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|█         | 1/10 [00:00<00:01,  4.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Negative (95%)]] --> [[[FAILED]]]\n",
            "\n",
            "hide new secretions from the parental units \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|██        | 2/10 [00:00<00:01,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[Negative (96%)]] --> [[[FAILED]]]\n",
            "\n",
            "contains no wit , only labored gags \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 3 / 1 / 4:  40%|████      | 4/10 [00:00<00:01,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[Positive (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "that loves its characters and communicates something rather beautiful about human nature \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[Positive (82%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "remains utterly satisfied to remain the same throughout \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 5 / 1 / 6:  60%|██████    | 6/10 [00:01<00:00,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[Negative (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "that 's far too tragic to merit such superficial treatment \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 7 / 1 / 8:  80%|████████  | 8/10 [00:01<00:00,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[Positive (90%)]] --> [[[FAILED]]]\n",
            "\n",
            "of saucy \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 8 / 1 / 9:  90%|█████████ | 9/10 [00:02<00:00,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "a depressed fifteen-year-old 's suicidal poetry \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 9 / 1 / 10: 100%|██████████| 10/10 [00:02<00:00,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[Positive (79%)]] --> [[[FAILED]]]\n",
            "\n",
            "are more deeply thought through than in most ` right-thinking ' films \n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+-------+\n",
            "| Attack Results                |       |\n",
            "+-------------------------------+-------+\n",
            "| Number of successful attacks: | 0     |\n",
            "| Number of failed attacks:     | 9     |\n",
            "| Number of skipped attacks:    | 1     |\n",
            "| Original accuracy:            | 90.0% |\n",
            "| Accuracy under attack:        | 90.0% |\n",
            "| Attack success rate:          | 0.0%  |\n",
            "| Average perturbed word %:     | nan%  |\n",
            "| Average num. words per input: | 9.5   |\n",
            "| Avg num queries:              | 60.44 |\n",
            "+-------------------------------+-------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/textattack/metrics/attack_metrics/words_perturbed.py:83: RuntimeWarning: Mean of empty slice.\n",
            "  average_perc_words_perturbed = self.perturbed_word_percentages.mean()\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9706021d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe972b68350>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7feb93106a90>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7fe97d686350>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9701cab10>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe970c833d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe982dd62d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9b5b61250>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe971df5190>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7feb93114590>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Recipe4(Attack,ABC):\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        transformation = WordSwapNeighboringCharacterSwap()\n",
        "\n",
        "        constraints = [MaxWordIndexModification(5)]\n",
        "\n",
        "        goal_function = InputReduction(model_wrapper)\n",
        "        # search over words based on a combination of their saliency score, and how efficient the WordSwap transform is\n",
        "        search_method = GreedyWordSwapWIR(wir_method=\"weighted-saliency\")\n",
        "        return Attack(goal_function, constraints, transformation, search_method)\n",
        "attack = Recipe4.build(model_wrapper)\n",
        "attacker = Attacker(attack, dataset)\n",
        "attacker.attack_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzOo9va0Izx7",
        "outputId": "87771a4f-efe1-4938-b908-d745a72e2851"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'allennlp.models.basic_classifier.BasicClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.input_reduction.InputReduction'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  weighted-saliency\n",
            "  )\n",
            "  (goal_function):  InputReduction(\n",
            "    (maximizable):  False\n",
            "    (target_num_words):  1\n",
            "  )\n",
            "  (transformation):  WordSwapNeighboringCharacterSwap(\n",
            "    (random_one):  True\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): MaxWordIndexModification(\n",
            "        (max_length):  5\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|██        | 2/10 [00:00<00:00, 11.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Negative (95%)]] --> [[[FAILED]]]\n",
            "\n",
            "hide new secretions from the parental units \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[Negative (96%)]] --> [[[FAILED]]]\n",
            "\n",
            "contains no wit , only labored gags \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 3 / 1 / 4:  40%|████      | 4/10 [00:00<00:00, 13.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[Positive (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "that loves its characters and communicates something rather beautiful about human nature \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[Positive (82%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "remains utterly satisfied to remain the same throughout \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 5 / 1 / 6:  60%|██████    | 6/10 [00:00<00:00,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[Negative (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "that 's far too tragic to merit such superficial treatment \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 9 / 1 / 10: 100%|██████████| 10/10 [00:01<00:00,  9.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[[FAILED]]]\n",
            "\n",
            "demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[Positive (90%)]] --> [[[FAILED]]]\n",
            "\n",
            "of saucy \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[Negative (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "a depressed fifteen-year-old 's suicidal poetry \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[Positive (79%)]] --> [[[FAILED]]]\n",
            "\n",
            "are more deeply thought through than in most ` right-thinking ' films \n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+-------+\n",
            "| Attack Results                |       |\n",
            "+-------------------------------+-------+\n",
            "| Number of successful attacks: | 0     |\n",
            "| Number of failed attacks:     | 9     |\n",
            "| Number of skipped attacks:    | 1     |\n",
            "| Original accuracy:            | 90.0% |\n",
            "| Accuracy under attack:        | 90.0% |\n",
            "| Attack success rate:          | 0.0%  |\n",
            "| Average perturbed word %:     | nan%  |\n",
            "| Average num. words per input: | 9.5   |\n",
            "| Avg num queries:              | 19.11 |\n",
            "+-------------------------------+-------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/textattack/metrics/attack_metrics/words_perturbed.py:83: RuntimeWarning: Mean of empty slice.\n",
            "  average_perc_words_perturbed = self.perturbed_word_percentages.mean()\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97bfefb50>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97fde2890>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97c7e6d10>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7fe982dd6cd0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97cdc39d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97b88fe90>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe97ae2bf10>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe975ccecd0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe9aea56f10>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fe982dd6750>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0av-2lHpejrW",
        "lrq7Mk7seR2N",
        "Y3QOjBI1c4kT",
        "-FIldZfrdIDm"
      ],
      "machine_shape": "hm",
      "name": "Picking_the_best-2_Allen-NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrHFaizR/fqK5kBDei7Lsl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c81526aaa3a4671bbe777f9b6c0a918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c23a3c3f54f146a7b43d58608f0962ff",
              "IPY_MODEL_8c2b1c6115164a239f7f3021a6100b5f",
              "IPY_MODEL_23d7702ae07f40d2ab72ff27b0fdb359"
            ],
            "layout": "IPY_MODEL_7122d9e6bd8442b2b7dd7142c0943a3e"
          }
        },
        "c23a3c3f54f146a7b43d58608f0962ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a78dbd19914a498b865c541137a78c",
            "placeholder": "​",
            "style": "IPY_MODEL_bd2402f291eb47feba67a130fd088c0c",
            "value": "100%"
          }
        },
        "8c2b1c6115164a239f7f3021a6100b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f4958b6e8547a9810adf67c6844569",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61e0b489a3e6400f95f3cc1eecec1eb6",
            "value": 3
          }
        },
        "23d7702ae07f40d2ab72ff27b0fdb359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3179723d6a5a4c1f98868ef133638c68",
            "placeholder": "​",
            "style": "IPY_MODEL_1d6c84b54a8c44e3b678c14767f1cc1e",
            "value": " 3/3 [00:00&lt;00:00, 97.28it/s]"
          }
        },
        "7122d9e6bd8442b2b7dd7142c0943a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a78dbd19914a498b865c541137a78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd2402f291eb47feba67a130fd088c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f4958b6e8547a9810adf67c6844569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61e0b489a3e6400f95f3cc1eecec1eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3179723d6a5a4c1f98868ef133638c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d6c84b54a8c44e3b678c14767f1cc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}